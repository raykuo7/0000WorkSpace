{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_angles(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    計算由三個座標形成的三角形的三個角度。\n",
    "\n",
    "    參數：\n",
    "    p1, p2, p3: tuple，代表三個座標 (x, y)\n",
    "\n",
    "    返回：\n",
    "    tuple，三角形的三個角度 (角1, 角2, 角3)，單位為度。\n",
    "    \"\"\"\n",
    "    def distance(a, b):\n",
    "        # 計算兩點之間的距離\n",
    "        return math.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n",
    "\n",
    "    # 計算三邊長\n",
    "    a = distance(p2, p3)  # 邊a對應角點p1\n",
    "    b = distance(p1, p3)  # 邊b對應角點p2\n",
    "    c = distance(p1, p2)  # 邊c對應角點p3\n",
    "\n",
    "    # 使用餘弦定理計算角度\n",
    "    angle1 = math.acos((b**2 + c**2 - a**2) / (2 * b * c))  # p1的角\n",
    "    angle2 = math.acos((a**2 + c**2 - b**2) / (2 * a * c))  # p2的角\n",
    "    angle3 = math.acos((a**2 + b**2 - c**2) / (2 * a * b))  # p3的角\n",
    "\n",
    "    # 將角度從弧度轉換為度\n",
    "    angle1 = math.degrees(angle1)\n",
    "    angle2 = math.degrees(angle2)\n",
    "    angle3 = math.degrees(angle3)\n",
    "\n",
    "    return angle1, angle2, angle3 ,a , b , c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12.433847185185085, 153.72764502265716, 13.838507792157724, 700.3456289575884, 1439.7694259845914, 778.0)\n"
     ]
    }
   ],
   "source": [
    "p1 = (1807,1897)\n",
    "p2 = (1807,1119)\n",
    "p3 = (1497,491)\n",
    "print(calculate_angles(p1, p2, p3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ptg(p1,p2,p3):\n",
    "    angle1, angle2, angle3 ,len1 , len2 , len3 = calculate_angles(p1, p2, p3)\n",
    "\n",
    "    grade = 0\n",
    "    grade += (1 - (len3/len1 - 778.0/700.3456289575884)**2**.5) * 30\n",
    "    grade += (1 - (len3/len2 - 778.0/1439.7694259845914)**2**.5) * 30\n",
    "    grade += (1 - (1 - angle1/12.433847185185085)**2**.5) * 15\n",
    "    grade += (1 - (1 - angle2/153.72764502265716)**2**.5) * 15\n",
    "    grade += (1 - (1 - angle3/13.838507792157724)**2**.5) * 10\n",
    "\n",
    "    if  len2 < len1:\n",
    "        grade = 0\n",
    "\n",
    "    \n",
    "    return grade\n",
    "\n",
    "\n",
    "print(ptg(p2,p1,p3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptg(p1, p2, p3):\n",
    "    angle1, angle2, angle3, len1, len2, len3 = calculate_angles(p1, p2, p3)\n",
    "\n",
    "    grade = 0\n",
    "    # 確保公式內的值非負，避免計算複數\n",
    "    grade1 = (1 - abs((len3 / len1) - (778.0 / 700.3456289575884))**0.5) * 30\n",
    "    grade1 = max(0, grade1)  # 避免分數變為負數\n",
    "    grade += grade1\n",
    "\n",
    "    grade2 = (1 - abs((len3 / len2) - (778.0 / 1439.7694259845914))**0.5) * 30\n",
    "    grade2 = max(0, grade2)\n",
    "    grade += grade2\n",
    "\n",
    "    grade3 = (1 - abs(1 - (angle1 / 12.433847185185085))**0.5) * 15\n",
    "    grade3 = max(0, grade3)\n",
    "    grade += grade3\n",
    "\n",
    "    grade4 = (1 - abs(1 - (angle2 / 153.72764502265716))**0.5) * 15\n",
    "    grade4 = max(0, grade4)\n",
    "    grade += grade4\n",
    "\n",
    "    grade5 = (1 - abs(1 - (angle3 / 13.838507792157724))**0.5) * 10\n",
    "    grade5 = max(0, grade5)\n",
    "    grade += grade5\n",
    "\n",
    "    # 確保邏輯條件正確\n",
    "    if len2 < len1:\n",
    "        grade = 0\n",
    "\n",
    "    # return grade, grade1, grade2, grade3, grade4, grade5\n",
    "    \n",
    "    return grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNPointDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNPointDetector, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # Output (x, y) coordinates\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(-1, 64 * 8 * 8)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_1 loaded and ready for inference.\n",
      "Model_2 loaded and ready for inference.\n",
      "Model_3 loaded and ready for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raykuo7\\AppData\\Local\\Temp\\ipykernel_21740\\3409150898.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_1.load_state_dict(torch.load(\"fan_corner1_detector_1201.pth\"))\n",
      "C:\\Users\\raykuo7\\AppData\\Local\\Temp\\ipykernel_21740\\3409150898.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_2.load_state_dict(torch.load(\"fan_corner2_detector_1201.pth\"))\n",
      "C:\\Users\\raykuo7\\AppData\\Local\\Temp\\ipykernel_21740\\3409150898.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_3.load_state_dict(torch.load(\"fan_corner3_detector_1201.pth\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 初始化模型\n",
    "model_1 = CNNPointDetector()\n",
    "model_1.load_state_dict(torch.load(\"fan_corner1_detector_1201.pth\"))\n",
    "model_1.eval()  # 切换到评估模式\n",
    "print(\"Model_1 loaded and ready for inference.\")\n",
    "\n",
    "model_2 = CNNPointDetector()\n",
    "model_2.load_state_dict(torch.load(\"fan_corner2_detector_1201.pth\"))\n",
    "model_2.eval()  # 切换到评估模式\n",
    "print(\"Model_2 loaded and ready for inference.\")\n",
    "\n",
    "model_3 = CNNPointDetector()\n",
    "model_3.load_state_dict(torch.load(\"fan_corner3_detector_1201.pth\"))\n",
    "model_3.eval()  # 切换到评估模式\n",
    "print(\"Model_3 loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]) Grade:  0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# image_files = glob.glob(r'C:\\GitHub\\fantest_pic\\try1128\\*.png')\n",
    "\n",
    "# if not image_files:\n",
    "#     print(\"No images found. Please check the path and try again.\")\n",
    "#     sys.exit()\n",
    "\n",
    "class FanDatasetFromPath(Dataset):\n",
    "    def __init__(self, image_folder, img_size=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (str): 路径到包含图片的文件夹\n",
    "            img_size (int): 调整图像大小的目标尺寸\n",
    "        \"\"\"\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        # 读取图片并转为灰度图\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size)) / 255.0  # Resize and normalize to [0, 1]\n",
    "        image = image.reshape(1, self.img_size, self.img_size)  # Add channel dimension\n",
    "\n",
    "        # 标签设为零向量（根据需求调整）\n",
    "        label = np.zeros(2, dtype=np.float32)  # Placeholder for labels (e.g., [x, y] coordinates)\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# 使用图片路径加载数据集\n",
    "image_folder_path = r'C:\\GitHub\\fantest_pic\\try1128'  # 替换为你的图片文件夹路径\n",
    "dataset = FanDatasetFromPath(image_folder_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(dataset)):\n",
    "            img, label = dataset[i]\n",
    "            img = img.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "            p1 = model_1(img)\n",
    "            p2 = model_2(img)\n",
    "            p3 = model_3(img)\n",
    "\n",
    "            p1 = (p1.numpy().flatten()[0] , p1.numpy().flatten()[1])\n",
    "            p2 = (p2.numpy().flatten()[0] , p2.numpy().flatten()[1])\n",
    "            p3 = (p3.numpy().flatten()[0] , p3.numpy().flatten()[1])\n",
    "    print(img , \"Grade: \", ptg(p1,p2,p3))\n",
    "    \n",
    "            \n",
    "# for image_path in image_files:\n",
    "    # image = cv2.imread(image_path)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),\n",
    "    # transforms.Resize((2047, 2047)),\n",
    "    # transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5], std=[0.5])  # 单通道归一化\n",
    "    # ])\n",
    "\n",
    "    # transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),\n",
    "    # transforms.Resize((2047, 2047)),  # 替换为你的模型输入尺寸\n",
    "    # transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 如果训练时有不同的归一化设置，请替换\n",
    "    # ])\n",
    "\n",
    "    # input_tensor = transform(image)\n",
    "    # input_tensor = input_tensor.unsqueeze(0) # 增加 batch 维度\n",
    "\n",
    "                # with torch.no_grad():  # 不需要梯度计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 数据集类定义\n",
    "class FanDatasetFromPath(Dataset):\n",
    "    def __init__(self, image_folder, img_size=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (str): 包含图片的文件夹路径\n",
    "            img_size (int): 调整图像大小的目标尺寸\n",
    "        \"\"\"\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        # 读取图片并转为灰度图\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"无法读取图片：{img_path}\")\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size)) / 2047.0  # Resize and normalize to [0, 1]\n",
    "        image = image.reshape(1, self.img_size, self.img_size)  # Add channel dimension\n",
    "        return torch.tensor(image, dtype=torch.float32), img_path\n",
    "\n",
    "# 加载数据集\n",
    "image_folder_path = r'D:\\FAN_CAM\\fanpic_1121\\test'  # 替换为你的图片文件夹路径\n",
    "dataset = FanDatasetFromPath(image_folder_path)\n",
    "\n",
    "# 遍历数据集并使用模型检测点\n",
    "with torch.no_grad():\n",
    "    for i in range(len(dataset)):\n",
    "        img, img_path = dataset[i]\n",
    "        img = img.unsqueeze(0)  # 增加 batch 维度\n",
    "\n",
    "        # 使用模型预测点\n",
    "        p1 = model_1(img)\n",
    "        p2 = model_2(img)\n",
    "        p3 = model_3(img)\n",
    "\n",
    "        print(f\"Type of p1: {type(p1)}, Content: {p1}\")\n",
    "        # print(f\"Type of p2: {type(p2)}, Content: {p2}\")\n",
    "        # print(f\"Type of p3: {type(p3)}, Content: {p3}\")\n",
    "        # # 转换输出为坐标\n",
    "        p1_coords = p1.squeeze().numpy()  # 转为 NumPy 数组并去掉多余维度\n",
    "        p2_coords = p2.squeeze().numpy()\n",
    "        p3_coords = p3.squeeze().numpy()\n",
    "\n",
    "        # 打印或记录结果\n",
    "        print(f\"Image: {img_path}\")\n",
    "        print(f\"Point 1: {p1_coords}, Point 2: {p2_coords}, Point 3: {p3_coords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_10_.jpg Grade:  0\n",
      "0_10_150.png Grade:  0\n",
      "0_10_180.png Grade:  0\n",
      "0_10_30.png Grade:  0\n",
      "0_10_300.png Grade:  0\n",
      "0_10_330.png Grade:  0\n",
      "0_11_210.png Grade:  0\n",
      "0_11_240.png Grade:  0\n",
      "0_11_360.png Grade:  41.8843041671942\n",
      "0_11_60.png Grade:  21.27280843846431\n",
      "0_11_90.png Grade:  0\n",
      "0_1_210.png Grade:  30.72869481881416\n",
      "0_1_240.png Grade:  30.094620422656607\n",
      "0_1_360.png Grade:  0\n",
      "0_1_60.png Grade:  0\n",
      "0_1_90.png Grade:  0\n",
      "0_2_.jpg Grade:  0\n",
      "0_2_120.png Grade:  0\n",
      "0_2_150.png Grade:  0\n",
      "0_2_270.png Grade:  0\n",
      "0_2_300.png Grade:  34.604831183466914\n",
      "0_3_180.png Grade:  0\n",
      "0_3_210.png Grade:  0\n",
      "0_3_30.png Grade:  20.011694585516256\n",
      "0_3_330.png Grade:  0\n",
      "0_3_360.png Grade:  0\n",
      "0_3_60.png Grade:  0\n",
      "0_4_120.png Grade:  0\n",
      "0_4_240.png Grade:  21.573780325951077\n",
      "0_4_270.png Grade:  0\n",
      "0_4_90.png Grade:  0\n",
      "0_5_.jpg Grade:  0\n",
      "0_5_150.png Grade:  35.86259296690683\n",
      "0_5_180.png Grade:  32.908974375527194\n",
      "0_5_30.png Grade:  17.97774546197498\n",
      "0_5_300.png Grade:  0\n",
      "0_5_330.png Grade:  0\n",
      "0_6_210.png Grade:  32.66005206505022\n",
      "0_6_240.png Grade:  22.233779827642078\n",
      "0_6_360.png Grade:  0\n",
      "0_6_60.png Grade:  0\n",
      "0_6_90.png Grade:  0\n",
      "0_7_.jpg Grade:  0\n",
      "0_7_120.png Grade:  0\n",
      "0_7_150.png Grade:  0\n",
      "0_7_270.png Grade:  0\n",
      "0_7_300.png Grade:  0\n",
      "0_8_180.png Grade:  0\n",
      "0_8_210.png Grade:  31.793152773990368\n",
      "0_8_30.png Grade:  24.293474778706923\n",
      "0_8_330.png Grade:  0\n",
      "0_8_360.png Grade:  0\n",
      "0_8_60.png Grade:  0\n",
      "0_9_120.png Grade:  0\n",
      "0_9_240.png Grade:  28.959994867369232\n",
      "0_9_270.png Grade:  0\n",
      "0_9_90.png Grade:  0\n",
      "1_104.png Grade:  76.87830142826152\n",
      "1_129.png Grade:  74.55701056679544\n",
      "1_134.png Grade:  52.95085702786052\n",
      "1_139.png Grade:  61.010695299344675\n",
      "1_164.png Grade:  67.37080079474349\n",
      "1_169.png Grade:  53.4388242495619\n",
      "1_174.png Grade:  68.34706977378626\n",
      "1_199.png Grade:  64.93752342087359\n",
      "1_204.png Grade:  47.5114572287393\n",
      "1_209.png Grade:  56.654411371284944\n",
      "1_234.png Grade:  75.6269803352299\n",
      "1_239.png Grade:  69.6326384934733\n",
      "1_24.png Grade:  35.49648965753984\n",
      "1_244.png Grade:  79.20545522713667\n",
      "1_269.png Grade:  60.977666236364115\n",
      "1_274.png Grade:  55.55682112709573\n",
      "1_279.png Grade:  45.294536674982126\n",
      "1_29.png Grade:  0\n",
      "1_304.png Grade:  62.6960934296954\n",
      "1_309.png Grade:  56.08670737374765\n",
      "1_314.png Grade:  57.467336194131406\n",
      "1_339.png Grade:  49.259397489541776\n",
      "1_34.png Grade:  30.143230678884223\n",
      "1_344.png Grade:  38.80560968102648\n",
      "1_349.png Grade:  42.96978048056441\n",
      "1_59.png Grade:  39.6401184952844\n",
      "1_64.png Grade:  36.437067164676776\n",
      "1_69.png Grade:  38.11990012875092\n",
      "1_94.png Grade:  50.44671127540835\n",
      "1_99.png Grade:  52.03438211908725\n"
     ]
    }
   ],
   "source": [
    "# 1130 \n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 数据集类定义\n",
    "class FanDatasetFromPath(Dataset):\n",
    "    def __init__(self, image_folder, img_size=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (str): 包含图片的文件夹路径\n",
    "            img_size (int): 调整图像大小的目标尺寸\n",
    "        \"\"\"\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        # 读取图片并转为灰度图\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"无法读取图片：{img_path}\")\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size)) / 2047.0  # Resize and normalize to [0, 1]\n",
    "        image = image.reshape(1, self.img_size, self.img_size)  # Add channel dimension\n",
    "        return torch.tensor(image, dtype=torch.float32), img_path\n",
    "\n",
    "# 加载数据集\n",
    "image_folder_path = r'C:\\GitHub\\fantest_pic\\try1128'  # 替换为你的图片文件夹路径\n",
    "dataset = FanDatasetFromPath(image_folder_path)\n",
    "\n",
    "# 遍历数据集并使用模型检测点\n",
    "with torch.no_grad():\n",
    "    for i in range(len(dataset)):\n",
    "        img, img_path = dataset[i]\n",
    "        img = img.unsqueeze(0)  # 增加 batch 维度\n",
    "        img_path = dataset.image_paths[i]\n",
    "        img_name = os.path.basename(img_path)\n",
    "        # 使用模型预测点\n",
    "        p1 = model_1(img)\n",
    "        p2 = model_2(img)\n",
    "        p3 = model_3(img)\n",
    "\n",
    "        p1 = tuple(p1.squeeze().tolist())\n",
    "        p2 = tuple(p2.squeeze().tolist())\n",
    "        p3 = tuple(p3.squeeze().tolist())\n",
    "        print(img_name , \"Grade: \", ptg(p1,p2,p3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_10_.jpg Grade:  0\n",
      "0_10_150.png Grade:  0\n",
      "0_10_180.png Grade:  0\n",
      "0_10_30.png Grade:  0\n",
      "0_10_300.png Grade:  0\n",
      "0_10_330.png Grade:  0\n",
      "0_11_210.png Grade:  0\n",
      "0_11_240.png Grade:  0\n",
      "0_11_360.png Grade:  0\n",
      "0_11_60.png Grade:  23.480343163696894\n",
      "0_11_90.png Grade:  0\n",
      "0_1_210.png Grade:  29.903686622169317\n",
      "0_1_240.png Grade:  0\n",
      "0_1_360.png Grade:  0\n",
      "0_1_60.png Grade:  0\n",
      "0_1_90.png Grade:  0\n",
      "0_2_.jpg Grade:  0\n",
      "0_2_120.png Grade:  0\n",
      "0_2_150.png Grade:  0\n",
      "0_2_270.png Grade:  0\n",
      "0_2_300.png Grade:  0\n",
      "0_3_180.png Grade:  0\n",
      "0_3_210.png Grade:  0\n",
      "0_3_30.png Grade:  20.649708475381424\n",
      "0_3_330.png Grade:  0\n",
      "0_3_360.png Grade:  0\n",
      "0_3_60.png Grade:  16.90581545911109\n",
      "0_4_120.png Grade:  0\n",
      "0_4_240.png Grade:  0\n",
      "0_4_270.png Grade:  0\n",
      "0_4_90.png Grade:  0\n",
      "0_5_.jpg Grade:  0\n",
      "0_5_150.png Grade:  34.972664837525215\n",
      "0_5_180.png Grade:  0\n",
      "0_5_30.png Grade:  27.43147654715662\n",
      "0_5_300.png Grade:  0\n",
      "0_5_330.png Grade:  0\n",
      "0_6_210.png Grade:  29.86110409524927\n",
      "0_6_240.png Grade:  0\n",
      "0_6_360.png Grade:  0\n",
      "0_6_60.png Grade:  17.857678600851568\n",
      "0_6_90.png Grade:  0\n",
      "0_7_.jpg Grade:  0\n",
      "0_7_120.png Grade:  0\n",
      "0_7_150.png Grade:  0\n",
      "0_7_270.png Grade:  0\n",
      "0_7_300.png Grade:  0\n",
      "0_8_180.png Grade:  0\n",
      "0_8_210.png Grade:  30.659029033685307\n",
      "0_8_30.png Grade:  29.42744311304807\n",
      "0_8_330.png Grade:  0\n",
      "0_8_360.png Grade:  0\n",
      "0_8_60.png Grade:  22.31716980353328\n",
      "0_9_120.png Grade:  0\n",
      "0_9_240.png Grade:  0\n",
      "0_9_270.png Grade:  0\n",
      "0_9_90.png Grade:  0\n",
      "1_104.png Grade:  66.4054608777674\n",
      "1_129.png Grade:  68.16876614606164\n",
      "1_134.png Grade:  55.26537444390183\n",
      "1_139.png Grade:  83.96593790495979\n",
      "1_164.png Grade:  60.436950135325056\n",
      "1_169.png Grade:  62.24232861257664\n",
      "1_174.png Grade:  72.63558082179601\n",
      "1_199.png Grade:  64.25728299046222\n",
      "1_204.png Grade:  57.30227117293674\n",
      "1_209.png Grade:  75.29486683235258\n",
      "1_234.png Grade:  77.775858384081\n",
      "1_239.png Grade:  71.93508956198971\n",
      "1_24.png Grade:  0\n",
      "1_244.png Grade:  70.87980228753685\n",
      "1_269.png Grade:  57.03060198987362\n",
      "1_274.png Grade:  77.92809173496698\n",
      "1_279.png Grade:  73.03083463770052\n",
      "1_29.png Grade:  0\n",
      "1_304.png Grade:  68.42274240349265\n",
      "1_309.png Grade:  81.2222417065454\n",
      "1_314.png Grade:  69.593978925449\n",
      "1_339.png Grade:  64.75444711608021\n",
      "1_34.png Grade:  29.891827947532168\n",
      "1_344.png Grade:  83.75524710520656\n",
      "1_349.png Grade:  77.35386156065393\n",
      "1_59.png Grade:  31.63052281054825\n",
      "1_64.png Grade:  28.336667509194804\n",
      "1_69.png Grade:  29.34062107838439\n",
      "1_94.png Grade:  57.39529077149863\n",
      "1_99.png Grade:  68.03012484671324\n"
     ]
    }
   ],
   "source": [
    "# 1201\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 数据集类定义\n",
    "class FanDatasetFromPath(Dataset):\n",
    "    def __init__(self, image_folder, img_size=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (str): 包含图片的文件夹路径\n",
    "            img_size (int): 调整图像大小的目标尺寸\n",
    "        \"\"\"\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        # 读取图片并转为灰度图\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"无法读取图片：{img_path}\")\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size)) / 2047.0  # Resize and normalize to [0, 1]\n",
    "        image = image.reshape(1, self.img_size, self.img_size)  # Add channel dimension\n",
    "        return torch.tensor(image, dtype=torch.float32), img_path\n",
    "\n",
    "# 加载数据集\n",
    "image_folder_path = r'C:\\GitHub\\fantest_pic\\try1128'  # 替换为你的图片文件夹路径\n",
    "dataset = FanDatasetFromPath(image_folder_path)\n",
    "\n",
    "# 遍历数据集并使用模型检测点\n",
    "with torch.no_grad():\n",
    "    for i in range(len(dataset)):\n",
    "        img, img_path = dataset[i]\n",
    "        img = img.unsqueeze(0)  # 增加 batch 维度\n",
    "        img_path = dataset.image_paths[i]\n",
    "        img_name = os.path.basename(img_path)\n",
    "        # 使用模型预测点\n",
    "        p1 = model_1(img)\n",
    "        p2 = model_2(img)\n",
    "        p3 = model_3(img)\n",
    "\n",
    "        p1 = tuple(p1.squeeze().tolist())\n",
    "        p2 = tuple(p2.squeeze().tolist())\n",
    "        p3 = tuple(p3.squeeze().tolist())\n",
    "        print(img_name , \"Grade: \", ptg(p1,p2,p3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_10_.jpg       Grade: 0.000     NO GO   0\n",
      "0_10_150.png    Grade: 28.305    NO GO   1\n",
      "0_10_180.png    Grade: 0.000     NO GO   2\n",
      "0_10_30.png     Grade: 18.292    NO GO   3\n",
      "0_10_300.png    Grade: 0.000     NO GO   4\n",
      "0_10_330.png    Grade: 0.000     NO GO   5\n",
      "0_11_210.png    Grade: 0.000     NO GO   6\n",
      "0_11_240.png    Grade: 0.000     NO GO   7\n",
      "0_11_360.png    Grade: 0.000     NO GO   8\n",
      "0_11_60.png     Grade: 28.565    NO GO   9\n",
      "0_11_90.png     Grade: 0.000     NO GO   10\n",
      "0_1_210.png     Grade: 31.785    NO GO   11\n",
      "0_1_240.png     Grade: 24.657    NO GO   12\n",
      "0_1_360.png     Grade: 0.000     NO GO   13\n",
      "0_1_60.png      Grade: 24.051    NO GO   14\n",
      "0_1_90.png      Grade: 0.000     NO GO   15\n",
      "0_2_.jpg        Grade: 0.000     NO GO   16\n",
      "0_2_120.png     Grade: 0.000     NO GO   17\n",
      "0_2_150.png     Grade: 0.000     NO GO   18\n",
      "0_2_270.png     Grade: 0.000     NO GO   19\n",
      "0_2_300.png     Grade: 0.000     NO GO   20\n",
      "0_3_180.png     Grade: 0.000     NO GO   21\n",
      "0_3_210.png     Grade: 27.655    NO GO   22\n",
      "0_3_30.png      Grade: 23.051    NO GO   23\n",
      "0_3_330.png     Grade: 0.000     NO GO   24\n",
      "0_3_360.png     Grade: 0.000     NO GO   25\n",
      "0_3_60.png      Grade: 29.561    NO GO   26\n",
      "0_4_120.png     Grade: 0.000     NO GO   27\n",
      "0_4_240.png     Grade: 0.000     NO GO   28\n",
      "0_4_270.png     Grade: 0.000     NO GO   29\n",
      "0_4_90.png      Grade: 0.000     NO GO   30\n",
      "0_5_.jpg        Grade: 0.000     NO GO   31\n",
      "0_5_150.png     Grade: 30.179    NO GO   32\n",
      "0_5_180.png     Grade: 0.000     NO GO   33\n",
      "0_5_30.png      Grade: 31.297    NO GO   34\n",
      "0_5_300.png     Grade: 0.000     NO GO   35\n",
      "0_5_330.png     Grade: 0.000     NO GO   36\n",
      "0_6_210.png     Grade: 31.266    NO GO   37\n",
      "0_6_240.png     Grade: 20.907    NO GO   38\n",
      "0_6_360.png     Grade: 0.000     NO GO   39\n",
      "0_6_60.png      Grade: 21.691    NO GO   40\n",
      "0_6_90.png      Grade: 0.000     NO GO   41\n",
      "0_7_.jpg        Grade: 0.000     NO GO   42\n",
      "0_7_120.png     Grade: 0.000     NO GO   43\n",
      "0_7_150.png     Grade: 0.000     NO GO   44\n",
      "0_7_270.png     Grade: 0.000     NO GO   45\n",
      "0_7_300.png     Grade: 0.000     NO GO   46\n",
      "0_8_180.png     Grade: 0.000     NO GO   47\n",
      "0_8_210.png     Grade: 31.180    NO GO   48\n",
      "0_8_30.png      Grade: 25.335    NO GO   49\n",
      "0_8_330.png     Grade: 18.930    NO GO   50\n",
      "0_8_360.png     Grade: 0.000     NO GO   51\n",
      "0_8_60.png      Grade: 18.933    NO GO   52\n",
      "0_9_120.png     Grade: 0.000     NO GO   53\n",
      "0_9_240.png     Grade: 24.496    NO GO   54\n",
      "0_9_270.png     Grade: 0.000     NO GO   55\n",
      "0_9_90.png      Grade: 0.000     NO GO   56\n",
      "1_104.png       Grade: 62.938    GO   57\n",
      "1_129.png       Grade: 77.688    GO   58\n",
      "1_134.png       Grade: 75.733    GO   59\n",
      "1_139.png       Grade: 74.211    GO   60\n",
      "1_164.png       Grade: 69.780    GO   61\n",
      "1_169.png       Grade: 57.426    GO   62\n",
      "1_174.png       Grade: 79.707    GO   63\n",
      "1_199.png       Grade: 66.003    GO   64\n",
      "1_204.png       Grade: 75.461    GO   65\n",
      "1_209.png       Grade: 73.499    GO   66\n",
      "1_234.png       Grade: 80.650    GO   67\n",
      "1_239.png       Grade: 89.214    GO   68\n",
      "1_24.png        Grade: 0.000     NO GO   69\n",
      "1_244.png       Grade: 65.969    GO   70\n",
      "1_269.png       Grade: 59.319    GO   71\n",
      "1_274.png       Grade: 68.323    GO   72\n",
      "1_279.png       Grade: 67.355    GO   73\n",
      "1_29.png        Grade: 0.000     NO GO   74\n",
      "1_304.png       Grade: 78.409    GO   75\n",
      "1_309.png       Grade: 76.300    GO   76\n",
      "1_314.png       Grade: 64.616    GO   77\n",
      "1_339.png       Grade: 68.511    GO   78\n",
      "1_34.png        Grade: 28.241    NO GO   79\n",
      "1_344.png       Grade: 81.848    GO   80\n",
      "1_349.png       Grade: 72.758    GO   81\n",
      "1_59.png        Grade: 32.682    NO GO   82\n",
      "1_64.png        Grade: 22.893    NO GO   83\n",
      "1_69.png        Grade: 26.685    NO GO   84\n",
      "1_94.png        Grade: 80.910    GO   85\n",
      "1_99.png        Grade: 83.679    GO   86\n"
     ]
    }
   ],
   "source": [
    "# 1202\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 数据集类定义\n",
    "class FanDatasetFromPath(Dataset):\n",
    "    def __init__(self, image_folder, img_size=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (str): 包含图片的文件夹路径\n",
    "            img_size (int): 调整图像大小的目标尺寸\n",
    "        \"\"\"\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        # 读取图片并转为灰度图\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"无法读取图片：{img_path}\")\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size)) / 2047.0  # Resize and normalize to [0, 1]\n",
    "        image = image.reshape(1, self.img_size, self.img_size)  # Add channel dimension\n",
    "        return torch.tensor(image, dtype=torch.float32), img_path\n",
    "\n",
    "# 加载数据集\n",
    "image_folder_path = r'C:\\GitHub\\fantest_pic\\try1128'  # 替换为你的图片文件夹路径\n",
    "dataset = FanDatasetFromPath(image_folder_path)\n",
    "\n",
    "# 遍历数据集并使用模型检测点\n",
    "with torch.no_grad():\n",
    "    for i in range(len(dataset)):\n",
    "        img, img_path = dataset[i]\n",
    "        img = img.unsqueeze(0)  # 增加 batch 维度\n",
    "        img_path = dataset.image_paths[i]\n",
    "        img_name = os.path.basename(img_path)\n",
    "        # 使用模型预测点\n",
    "        p1 = model_1(img)\n",
    "        p2 = model_2(img)\n",
    "        p3 = model_3(img)\n",
    "\n",
    "        p1 = tuple(p1.squeeze().tolist())\n",
    "        p2 = tuple(p2.squeeze().tolist())\n",
    "        p3 = tuple(p3.squeeze().tolist())\n",
    "        print('%-15s'%img_name , \"Grade: %-6.3f\" % ptg(p1,p2,p3), end='    ')\n",
    "        if ptg(p1,p2,p3) >=50:\n",
    "            print(\"GO\",\" \",i)\n",
    "        else:\n",
    "            print(\"NO GO\",\" \",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def add_prefix_to_files(folder_path, prefix=\"1_\"):\n",
    "    \"\"\"\n",
    "    將指定資料夾內所有檔案的名稱前加上指定的前綴。\n",
    "\n",
    "    參數：\n",
    "        folder_path (str): 資料夾的路徑。\n",
    "        prefix (str): 要添加到檔名前的字串，預設為 \"_1\"。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 確認資料夾存在\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"資料夾不存在: {folder_path}\")\n",
    "            return\n",
    "\n",
    "        # 遍歷資料夾內的所有檔案\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # 確保只處理檔案，忽略資料夾\n",
    "            if os.path.isfile(file_path):\n",
    "                new_name = prefix + filename\n",
    "                new_path = os.path.join(folder_path, new_name)\n",
    "\n",
    "                # 重命名檔案\n",
    "                os.rename(file_path, new_path)\n",
    "                print(f\"重命名: {filename} -> {new_name}\")\n",
    "\n",
    "        print(\"檔案重命名完成！\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"發生錯誤: {e}\")\n",
    "\n",
    "# 使用範例\n",
    "folder_path = \"D:/FAN_CAM/fanpic_1121/test\"  # 替換成目標資料夾路徑\n",
    "add_prefix_to_files(folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvdi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
