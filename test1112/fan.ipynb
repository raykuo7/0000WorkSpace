{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我想用python做一個程式:\n",
    "1. 批量讀入照片\n",
    "2.  依序顯示每張照片，並可以手動選取圖片上的點\n",
    "3. 記錄點座標並寫成 .csv檔\n",
    "4. .csv檔格式為 image_path, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=874, y=1251\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=759, y=894\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=885, y=1075\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=897, y=1095\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=909, y=1112\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=922, y=1131\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=932, y=1150\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=947, y=1165\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=957, y=1184\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=967, y=1198\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=981, y=1217\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=988, y=1232\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=774, y=915\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=999, y=1247\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1007, y=1262\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1015, y=1277\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1028, y=1292\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1038, y=1305\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1044, y=1318\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1054, y=1331\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1061, y=1345\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1070, y=1358\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1075, y=1371\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=791, y=935\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1082, y=1381\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1088, y=1394\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1097, y=1402\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1101, y=1411\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1108, y=1424\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1112, y=1434\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1117, y=1439\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1121, y=1448\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1125, y=1457\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1130, y=1464\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=808, y=959\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1135, y=1469\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1140, y=1477\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1140, y=1485\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1143, y=1488\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1143, y=1491\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1144, y=1503\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1144, y=1504\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1148, y=1509\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1148, y=1513\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1148, y=1518\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=819, y=978\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=1148, y=1518\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=831, y=998\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=846, y=1018\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=861, y=1036\n",
      "Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\n",
      "Point selected at: x=876, y=1056\n",
      "Data saved to selected_points.csv\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Define initial display scale factor\n",
    "scale_factor = 1.0\n",
    "min_scale = 0.1\n",
    "max_scale = 3.0\n",
    "\n",
    "# Callback function to record points\n",
    "def select_point(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        # Calculate original coordinates based on current scale factor\n",
    "        orig_x = int(x / scale_factor)\n",
    "        orig_y = int(y / scale_factor)\n",
    "        data.append((param, orig_x, orig_y))\n",
    "        print(f\"Point selected at: x={orig_x}, y={orig_y}\")\n",
    "\n",
    "# Load images from a directory (replace 'your_image_folder_path' with your folder path)\n",
    "image_files = glob.glob('.\\pic\\*.png')  # Adjust file extension as needed\n",
    "\n",
    "# Process each image\n",
    "for img_path in image_files:\n",
    "    # Read the original image\n",
    "    img = cv2.imread(img_path)\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Display the image with interactive scaling\n",
    "    while True:\n",
    "        # Resize image according to the current scale factor\n",
    "        display_img = cv2.resize(img, (int(w * scale_factor), int(h * scale_factor)))\n",
    "        \n",
    "        # Show the image and set the callback\n",
    "        cv2.imshow(\"Image\", display_img)\n",
    "        cv2.setMouseCallback(\"Image\", select_point, img_path)\n",
    "        \n",
    "        print(\"Click on the image to select a point. Use '+' to zoom in, '-' to zoom out, 'n' for next image.\")\n",
    "        key = cv2.waitKey(0)\n",
    "        \n",
    "        if key == ord('n'):\n",
    "            break\n",
    "        elif key == ord('+') and scale_factor < max_scale:\n",
    "            scale_factor += 0.1\n",
    "        elif key == ord('-') and scale_factor > min_scale:\n",
    "            scale_factor -= 0.1\n",
    "        elif key == ord('q'):  # Optional: press 'q' to quit early\n",
    "            break\n",
    "    \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df = pd.DataFrame(data, columns=[\"image_path\", \"x\", \"y\"])\n",
    "df.to_csv(\"selected_points.csv\", index=False)\n",
    "print(\"Data saved to selected_points.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class FanDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_size=64):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.annotations.iloc[idx, 0]\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size)) / 2047.0  # Resize and normalize\n",
    "        image = image.reshape(1, self.img_size, self.img_size)  # Reshape for CNN input\n",
    "\n",
    "        # Label: x and y coordinates of the corner\n",
    "        label = self.annotations.iloc[idx, 1:3].values.astype(np.float32)\n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# 使用自己的标注文件\n",
    "dataset = FanDataset('selected_points.csv')\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNPointDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNPointDetector, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # Output (x, y) coordinates\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(-1, 64 * 8 * 8)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = CNNPointDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 1184355.4375\n",
      "Epoch [2/1000], Loss: 1088865.6250\n",
      "Epoch [3/1000], Loss: 991240.9375\n",
      "Epoch [4/1000], Loss: 861901.9375\n",
      "Epoch [5/1000], Loss: 721252.0938\n",
      "Epoch [6/1000], Loss: 553222.9688\n",
      "Epoch [7/1000], Loss: 366246.7500\n",
      "Epoch [8/1000], Loss: 197410.6953\n",
      "Epoch [9/1000], Loss: 73881.1680\n",
      "Epoch [10/1000], Loss: 31509.8867\n",
      "Epoch [11/1000], Loss: 81084.6836\n",
      "Epoch [12/1000], Loss: 158630.5703\n",
      "Epoch [13/1000], Loss: 154425.6719\n",
      "Epoch [14/1000], Loss: 84663.8066\n",
      "Epoch [15/1000], Loss: 41562.8184\n",
      "Epoch [16/1000], Loss: 27468.2637\n",
      "Epoch [17/1000], Loss: 37901.6113\n",
      "Epoch [18/1000], Loss: 52145.6445\n",
      "Epoch [19/1000], Loss: 56986.0820\n",
      "Epoch [20/1000], Loss: 57230.7695\n",
      "Epoch [21/1000], Loss: 49032.2188\n",
      "Epoch [22/1000], Loss: 37042.7266\n",
      "Epoch [23/1000], Loss: 28662.0508\n",
      "Epoch [24/1000], Loss: 27054.1494\n",
      "Epoch [25/1000], Loss: 31395.0049\n",
      "Epoch [26/1000], Loss: 37415.1602\n",
      "Epoch [27/1000], Loss: 35384.4668\n",
      "Epoch [28/1000], Loss: 31872.3818\n",
      "Epoch [29/1000], Loss: 29880.4834\n",
      "Epoch [30/1000], Loss: 28759.2461\n",
      "Epoch [31/1000], Loss: 29452.6592\n",
      "Epoch [32/1000], Loss: 29299.9873\n",
      "Epoch [33/1000], Loss: 30669.9531\n",
      "Epoch [34/1000], Loss: 30677.4336\n",
      "Epoch [35/1000], Loss: 27351.7344\n",
      "Epoch [36/1000], Loss: 27502.5654\n",
      "Epoch [37/1000], Loss: 30641.1152\n",
      "Epoch [38/1000], Loss: 28237.1943\n",
      "Epoch [39/1000], Loss: 29171.0957\n",
      "Epoch [40/1000], Loss: 28802.5566\n",
      "Epoch [41/1000], Loss: 26253.6738\n",
      "Epoch [42/1000], Loss: 27406.5098\n",
      "Epoch [43/1000], Loss: 27835.5781\n",
      "Epoch [44/1000], Loss: 27301.2578\n",
      "Epoch [45/1000], Loss: 28744.1621\n",
      "Epoch [46/1000], Loss: 27097.7764\n",
      "Epoch [47/1000], Loss: 26151.3779\n",
      "Epoch [48/1000], Loss: 26138.7363\n",
      "Epoch [49/1000], Loss: 27979.1123\n",
      "Epoch [50/1000], Loss: 28457.9551\n",
      "Epoch [51/1000], Loss: 27866.4971\n",
      "Epoch [52/1000], Loss: 25924.8057\n",
      "Epoch [53/1000], Loss: 28266.0137\n",
      "Epoch [54/1000], Loss: 27061.0410\n",
      "Epoch [55/1000], Loss: 27430.2607\n",
      "Epoch [56/1000], Loss: 27168.8457\n",
      "Epoch [57/1000], Loss: 27811.3398\n",
      "Epoch [58/1000], Loss: 26098.6221\n",
      "Epoch [59/1000], Loss: 26179.3574\n",
      "Epoch [60/1000], Loss: 27603.1445\n",
      "Epoch [61/1000], Loss: 25699.6455\n",
      "Epoch [62/1000], Loss: 26956.3867\n",
      "Epoch [63/1000], Loss: 27280.0410\n",
      "Epoch [64/1000], Loss: 27846.7363\n",
      "Epoch [65/1000], Loss: 26568.0811\n",
      "Epoch [66/1000], Loss: 25339.9336\n",
      "Epoch [67/1000], Loss: 26718.7373\n",
      "Epoch [68/1000], Loss: 25237.1318\n",
      "Epoch [69/1000], Loss: 24626.1123\n",
      "Epoch [70/1000], Loss: 25779.3105\n",
      "Epoch [71/1000], Loss: 25682.9414\n",
      "Epoch [72/1000], Loss: 26148.0615\n",
      "Epoch [73/1000], Loss: 24758.0576\n",
      "Epoch [74/1000], Loss: 24259.8916\n",
      "Epoch [75/1000], Loss: 26897.1533\n",
      "Epoch [76/1000], Loss: 26681.3574\n",
      "Epoch [77/1000], Loss: 25428.1006\n",
      "Epoch [78/1000], Loss: 26092.7900\n",
      "Epoch [79/1000], Loss: 25635.6689\n",
      "Epoch [80/1000], Loss: 26673.3955\n",
      "Epoch [81/1000], Loss: 27131.8193\n",
      "Epoch [82/1000], Loss: 27796.6914\n",
      "Epoch [83/1000], Loss: 25097.7285\n",
      "Epoch [84/1000], Loss: 24565.5566\n",
      "Epoch [85/1000], Loss: 24694.6680\n",
      "Epoch [86/1000], Loss: 24726.9150\n",
      "Epoch [87/1000], Loss: 26511.4404\n",
      "Epoch [88/1000], Loss: 24689.7422\n",
      "Epoch [89/1000], Loss: 25229.7842\n",
      "Epoch [90/1000], Loss: 26296.3975\n",
      "Epoch [91/1000], Loss: 26435.4834\n",
      "Epoch [92/1000], Loss: 26608.3926\n",
      "Epoch [93/1000], Loss: 25004.1631\n",
      "Epoch [94/1000], Loss: 25546.2275\n",
      "Epoch [95/1000], Loss: 26580.7363\n",
      "Epoch [96/1000], Loss: 25026.7129\n",
      "Epoch [97/1000], Loss: 26098.1865\n",
      "Epoch [98/1000], Loss: 24916.1113\n",
      "Epoch [99/1000], Loss: 24009.2422\n",
      "Epoch [100/1000], Loss: 24978.5547\n",
      "Epoch [101/1000], Loss: 25151.2461\n",
      "Epoch [102/1000], Loss: 26113.7744\n",
      "Epoch [103/1000], Loss: 23988.6641\n",
      "Epoch [104/1000], Loss: 25880.4619\n",
      "Epoch [105/1000], Loss: 23557.8633\n",
      "Epoch [106/1000], Loss: 26351.5840\n",
      "Epoch [107/1000], Loss: 24314.1455\n",
      "Epoch [108/1000], Loss: 25736.7393\n",
      "Epoch [109/1000], Loss: 23894.4102\n",
      "Epoch [110/1000], Loss: 25250.8076\n",
      "Epoch [111/1000], Loss: 26619.3008\n",
      "Epoch [112/1000], Loss: 25062.1123\n",
      "Epoch [113/1000], Loss: 22047.1211\n",
      "Epoch [114/1000], Loss: 25154.2822\n",
      "Epoch [115/1000], Loss: 24651.9854\n",
      "Epoch [116/1000], Loss: 23195.3145\n",
      "Epoch [117/1000], Loss: 23695.7871\n",
      "Epoch [118/1000], Loss: 26076.6211\n",
      "Epoch [119/1000], Loss: 22670.0698\n",
      "Epoch [120/1000], Loss: 23136.3369\n",
      "Epoch [121/1000], Loss: 23377.9248\n",
      "Epoch [122/1000], Loss: 24135.4023\n",
      "Epoch [123/1000], Loss: 24914.4824\n",
      "Epoch [124/1000], Loss: 22805.8589\n",
      "Epoch [125/1000], Loss: 22883.6602\n",
      "Epoch [126/1000], Loss: 24015.3408\n",
      "Epoch [127/1000], Loss: 23336.5361\n",
      "Epoch [128/1000], Loss: 25487.4941\n",
      "Epoch [129/1000], Loss: 23449.1602\n",
      "Epoch [130/1000], Loss: 22864.0000\n",
      "Epoch [131/1000], Loss: 25367.0693\n",
      "Epoch [132/1000], Loss: 23294.0938\n",
      "Epoch [133/1000], Loss: 26751.0820\n",
      "Epoch [134/1000], Loss: 23842.3936\n",
      "Epoch [135/1000], Loss: 25817.3945\n",
      "Epoch [136/1000], Loss: 24087.6260\n",
      "Epoch [137/1000], Loss: 25065.3994\n",
      "Epoch [138/1000], Loss: 25354.7734\n",
      "Epoch [139/1000], Loss: 24701.9912\n",
      "Epoch [140/1000], Loss: 25928.3682\n",
      "Epoch [141/1000], Loss: 25056.7295\n",
      "Epoch [142/1000], Loss: 25247.1494\n",
      "Epoch [143/1000], Loss: 23583.9014\n",
      "Epoch [144/1000], Loss: 24804.1279\n",
      "Epoch [145/1000], Loss: 23762.1934\n",
      "Epoch [146/1000], Loss: 24827.3857\n",
      "Epoch [147/1000], Loss: 24211.2451\n",
      "Epoch [148/1000], Loss: 24418.0098\n",
      "Epoch [149/1000], Loss: 24816.3301\n",
      "Epoch [150/1000], Loss: 23966.5527\n",
      "Epoch [151/1000], Loss: 22963.2188\n",
      "Epoch [152/1000], Loss: 24143.2168\n",
      "Epoch [153/1000], Loss: 24499.8711\n",
      "Epoch [154/1000], Loss: 24316.8193\n",
      "Epoch [155/1000], Loss: 24369.1143\n",
      "Epoch [156/1000], Loss: 23433.7588\n",
      "Epoch [157/1000], Loss: 24487.6113\n",
      "Epoch [158/1000], Loss: 24183.9766\n",
      "Epoch [159/1000], Loss: 22811.3711\n",
      "Epoch [160/1000], Loss: 23756.5146\n",
      "Epoch [161/1000], Loss: 24216.4258\n",
      "Epoch [162/1000], Loss: 23823.7832\n",
      "Epoch [163/1000], Loss: 22939.7588\n",
      "Epoch [164/1000], Loss: 23110.4609\n",
      "Epoch [165/1000], Loss: 22695.8633\n",
      "Epoch [166/1000], Loss: 23109.9297\n",
      "Epoch [167/1000], Loss: 22434.9893\n",
      "Epoch [168/1000], Loss: 24450.3535\n",
      "Epoch [169/1000], Loss: 24097.6680\n",
      "Epoch [170/1000], Loss: 23142.7588\n",
      "Epoch [171/1000], Loss: 23130.9170\n",
      "Epoch [172/1000], Loss: 23882.2119\n",
      "Epoch [173/1000], Loss: 23603.6475\n",
      "Epoch [174/1000], Loss: 24126.7100\n",
      "Epoch [175/1000], Loss: 21500.7520\n",
      "Epoch [176/1000], Loss: 22330.3447\n",
      "Epoch [177/1000], Loss: 24422.7012\n",
      "Epoch [178/1000], Loss: 23619.0479\n",
      "Epoch [179/1000], Loss: 23353.3682\n",
      "Epoch [180/1000], Loss: 25180.9590\n",
      "Epoch [181/1000], Loss: 24876.5771\n",
      "Epoch [182/1000], Loss: 25322.0029\n",
      "Epoch [183/1000], Loss: 23434.4160\n",
      "Epoch [184/1000], Loss: 22467.1562\n",
      "Epoch [185/1000], Loss: 24097.1562\n",
      "Epoch [186/1000], Loss: 22988.6533\n",
      "Epoch [187/1000], Loss: 23107.7842\n",
      "Epoch [188/1000], Loss: 24173.0781\n",
      "Epoch [189/1000], Loss: 22860.0146\n",
      "Epoch [190/1000], Loss: 23302.1318\n",
      "Epoch [191/1000], Loss: 22563.4736\n",
      "Epoch [192/1000], Loss: 22907.6426\n",
      "Epoch [193/1000], Loss: 22066.2490\n",
      "Epoch [194/1000], Loss: 22083.4014\n",
      "Epoch [195/1000], Loss: 22170.2412\n",
      "Epoch [196/1000], Loss: 22006.9268\n",
      "Epoch [197/1000], Loss: 21753.0693\n",
      "Epoch [198/1000], Loss: 25057.8271\n",
      "Epoch [199/1000], Loss: 22497.5342\n",
      "Epoch [200/1000], Loss: 22693.6943\n",
      "Epoch [201/1000], Loss: 23207.1924\n",
      "Epoch [202/1000], Loss: 22229.6836\n",
      "Epoch [203/1000], Loss: 24186.2471\n",
      "Epoch [204/1000], Loss: 24114.7607\n",
      "Epoch [205/1000], Loss: 21752.5898\n",
      "Epoch [206/1000], Loss: 22194.9473\n",
      "Epoch [207/1000], Loss: 22905.0059\n",
      "Epoch [208/1000], Loss: 22331.5469\n",
      "Epoch [209/1000], Loss: 21488.9785\n",
      "Epoch [210/1000], Loss: 21853.6846\n",
      "Epoch [211/1000], Loss: 22242.6465\n",
      "Epoch [212/1000], Loss: 20880.9546\n",
      "Epoch [213/1000], Loss: 21459.5674\n",
      "Epoch [214/1000], Loss: 21624.0762\n",
      "Epoch [215/1000], Loss: 21250.6768\n",
      "Epoch [216/1000], Loss: 22136.4492\n",
      "Epoch [217/1000], Loss: 20810.6631\n",
      "Epoch [218/1000], Loss: 20606.2324\n",
      "Epoch [219/1000], Loss: 21503.4229\n",
      "Epoch [220/1000], Loss: 21678.1787\n",
      "Epoch [221/1000], Loss: 22401.8438\n",
      "Epoch [222/1000], Loss: 21910.4551\n",
      "Epoch [223/1000], Loss: 22477.6445\n",
      "Epoch [224/1000], Loss: 21371.1846\n",
      "Epoch [225/1000], Loss: 23323.2568\n",
      "Epoch [226/1000], Loss: 20944.4688\n",
      "Epoch [227/1000], Loss: 21773.6025\n",
      "Epoch [228/1000], Loss: 21378.7559\n",
      "Epoch [229/1000], Loss: 20445.2676\n",
      "Epoch [230/1000], Loss: 21535.7480\n",
      "Epoch [231/1000], Loss: 20935.2188\n",
      "Epoch [232/1000], Loss: 20468.6758\n",
      "Epoch [233/1000], Loss: 21447.2354\n",
      "Epoch [234/1000], Loss: 20401.8438\n",
      "Epoch [235/1000], Loss: 20729.9844\n",
      "Epoch [236/1000], Loss: 20471.0938\n",
      "Epoch [237/1000], Loss: 19207.7373\n",
      "Epoch [238/1000], Loss: 20034.0996\n",
      "Epoch [239/1000], Loss: 21117.2900\n",
      "Epoch [240/1000], Loss: 20765.8926\n",
      "Epoch [241/1000], Loss: 21719.5312\n",
      "Epoch [242/1000], Loss: 18847.8013\n",
      "Epoch [243/1000], Loss: 17931.3184\n",
      "Epoch [244/1000], Loss: 19212.6729\n",
      "Epoch [245/1000], Loss: 20048.7139\n",
      "Epoch [246/1000], Loss: 19917.7510\n",
      "Epoch [247/1000], Loss: 19388.4443\n",
      "Epoch [248/1000], Loss: 19738.1611\n",
      "Epoch [249/1000], Loss: 18291.7725\n",
      "Epoch [250/1000], Loss: 18271.6025\n",
      "Epoch [251/1000], Loss: 19517.0527\n",
      "Epoch [252/1000], Loss: 17462.0317\n",
      "Epoch [253/1000], Loss: 20034.3936\n",
      "Epoch [254/1000], Loss: 18635.9248\n",
      "Epoch [255/1000], Loss: 18441.1924\n",
      "Epoch [256/1000], Loss: 18907.4458\n",
      "Epoch [257/1000], Loss: 17995.2959\n",
      "Epoch [258/1000], Loss: 17828.7510\n",
      "Epoch [259/1000], Loss: 16961.2417\n",
      "Epoch [260/1000], Loss: 16974.7378\n",
      "Epoch [261/1000], Loss: 17555.2397\n",
      "Epoch [262/1000], Loss: 16549.2139\n",
      "Epoch [263/1000], Loss: 17345.3486\n",
      "Epoch [264/1000], Loss: 17648.6387\n",
      "Epoch [265/1000], Loss: 17225.2485\n",
      "Epoch [266/1000], Loss: 16525.2373\n",
      "Epoch [267/1000], Loss: 16121.2842\n",
      "Epoch [268/1000], Loss: 15639.0464\n",
      "Epoch [269/1000], Loss: 15750.3174\n",
      "Epoch [270/1000], Loss: 15944.7280\n",
      "Epoch [271/1000], Loss: 15453.8193\n",
      "Epoch [272/1000], Loss: 15576.8350\n",
      "Epoch [273/1000], Loss: 16114.3716\n",
      "Epoch [274/1000], Loss: 14478.8726\n",
      "Epoch [275/1000], Loss: 15030.2080\n",
      "Epoch [276/1000], Loss: 13844.6943\n",
      "Epoch [277/1000], Loss: 14361.2544\n",
      "Epoch [278/1000], Loss: 14280.4531\n",
      "Epoch [279/1000], Loss: 14086.4995\n",
      "Epoch [280/1000], Loss: 14369.3188\n",
      "Epoch [281/1000], Loss: 14019.8765\n",
      "Epoch [282/1000], Loss: 12990.3530\n",
      "Epoch [283/1000], Loss: 14103.8804\n",
      "Epoch [284/1000], Loss: 12344.2393\n",
      "Epoch [285/1000], Loss: 11651.7554\n",
      "Epoch [286/1000], Loss: 12419.3252\n",
      "Epoch [287/1000], Loss: 12153.0986\n",
      "Epoch [288/1000], Loss: 11932.4927\n",
      "Epoch [289/1000], Loss: 11323.3076\n",
      "Epoch [290/1000], Loss: 11139.3267\n",
      "Epoch [291/1000], Loss: 11106.5693\n",
      "Epoch [292/1000], Loss: 9701.9678\n",
      "Epoch [293/1000], Loss: 9095.5801\n",
      "Epoch [294/1000], Loss: 9653.8662\n",
      "Epoch [295/1000], Loss: 9289.0061\n",
      "Epoch [296/1000], Loss: 8184.4785\n",
      "Epoch [297/1000], Loss: 7513.4561\n",
      "Epoch [298/1000], Loss: 7105.9954\n",
      "Epoch [299/1000], Loss: 6899.3345\n",
      "Epoch [300/1000], Loss: 6580.2952\n",
      "Epoch [301/1000], Loss: 6165.4934\n",
      "Epoch [302/1000], Loss: 5499.7556\n",
      "Epoch [303/1000], Loss: 5052.8347\n",
      "Epoch [304/1000], Loss: 4580.0640\n",
      "Epoch [305/1000], Loss: 4286.9736\n",
      "Epoch [306/1000], Loss: 3869.6902\n",
      "Epoch [307/1000], Loss: 3624.5660\n",
      "Epoch [308/1000], Loss: 3183.1761\n",
      "Epoch [309/1000], Loss: 2891.5234\n",
      "Epoch [310/1000], Loss: 2706.8547\n",
      "Epoch [311/1000], Loss: 2777.6616\n",
      "Epoch [312/1000], Loss: 2451.7037\n",
      "Epoch [313/1000], Loss: 2457.3692\n",
      "Epoch [314/1000], Loss: 2069.6947\n",
      "Epoch [315/1000], Loss: 2368.9126\n",
      "Epoch [316/1000], Loss: 1793.1001\n",
      "Epoch [317/1000], Loss: 1813.8115\n",
      "Epoch [318/1000], Loss: 2027.1797\n",
      "Epoch [319/1000], Loss: 1709.3385\n",
      "Epoch [320/1000], Loss: 1600.7556\n",
      "Epoch [321/1000], Loss: 2003.0443\n",
      "Epoch [322/1000], Loss: 1913.8763\n",
      "Epoch [323/1000], Loss: 1521.6338\n",
      "Epoch [324/1000], Loss: 1454.1742\n",
      "Epoch [325/1000], Loss: 1458.9085\n",
      "Epoch [326/1000], Loss: 1944.8038\n",
      "Epoch [327/1000], Loss: 1372.7504\n",
      "Epoch [328/1000], Loss: 1791.6028\n",
      "Epoch [329/1000], Loss: 1314.6288\n",
      "Epoch [330/1000], Loss: 1293.8317\n",
      "Epoch [331/1000], Loss: 1348.0554\n",
      "Epoch [332/1000], Loss: 1348.9402\n",
      "Epoch [333/1000], Loss: 1276.1311\n",
      "Epoch [334/1000], Loss: 1196.0858\n",
      "Epoch [335/1000], Loss: 1615.4709\n",
      "Epoch [336/1000], Loss: 1276.8244\n",
      "Epoch [337/1000], Loss: 1566.3004\n",
      "Epoch [338/1000], Loss: 1129.1758\n",
      "Epoch [339/1000], Loss: 1565.2266\n",
      "Epoch [340/1000], Loss: 1172.8515\n",
      "Epoch [341/1000], Loss: 1477.4059\n",
      "Epoch [342/1000], Loss: 1521.5319\n",
      "Epoch [343/1000], Loss: 1051.4507\n",
      "Epoch [344/1000], Loss: 1094.3274\n",
      "Epoch [345/1000], Loss: 1361.7660\n",
      "Epoch [346/1000], Loss: 980.9045\n",
      "Epoch [347/1000], Loss: 1007.4852\n",
      "Epoch [348/1000], Loss: 975.3425\n",
      "Epoch [349/1000], Loss: 1012.3456\n",
      "Epoch [350/1000], Loss: 981.1779\n",
      "Epoch [351/1000], Loss: 1285.4470\n",
      "Epoch [352/1000], Loss: 1059.6018\n",
      "Epoch [353/1000], Loss: 958.7529\n",
      "Epoch [354/1000], Loss: 927.1484\n",
      "Epoch [355/1000], Loss: 950.0164\n",
      "Epoch [356/1000], Loss: 965.4339\n",
      "Epoch [357/1000], Loss: 902.4979\n",
      "Epoch [358/1000], Loss: 863.2676\n",
      "Epoch [359/1000], Loss: 894.9974\n",
      "Epoch [360/1000], Loss: 868.4795\n",
      "Epoch [361/1000], Loss: 1181.5632\n",
      "Epoch [362/1000], Loss: 1160.8353\n",
      "Epoch [363/1000], Loss: 832.4716\n",
      "Epoch [364/1000], Loss: 1183.9567\n",
      "Epoch [365/1000], Loss: 848.8549\n",
      "Epoch [366/1000], Loss: 1166.0084\n",
      "Epoch [367/1000], Loss: 821.5559\n",
      "Epoch [368/1000], Loss: 812.9068\n",
      "Epoch [369/1000], Loss: 812.6138\n",
      "Epoch [370/1000], Loss: 801.2935\n",
      "Epoch [371/1000], Loss: 1064.6841\n",
      "Epoch [372/1000], Loss: 805.8719\n",
      "Epoch [373/1000], Loss: 774.0518\n",
      "Epoch [374/1000], Loss: 769.3402\n",
      "Epoch [375/1000], Loss: 1032.6116\n",
      "Epoch [376/1000], Loss: 1053.9500\n",
      "Epoch [377/1000], Loss: 760.0782\n",
      "Epoch [378/1000], Loss: 768.2128\n",
      "Epoch [379/1000], Loss: 1082.3430\n",
      "Epoch [380/1000], Loss: 1066.8687\n",
      "Epoch [381/1000], Loss: 1045.6707\n",
      "Epoch [382/1000], Loss: 699.3448\n",
      "Epoch [383/1000], Loss: 723.6084\n",
      "Epoch [384/1000], Loss: 780.9171\n",
      "Epoch [385/1000], Loss: 972.8590\n",
      "Epoch [386/1000], Loss: 732.1831\n",
      "Epoch [387/1000], Loss: 725.1416\n",
      "Epoch [388/1000], Loss: 725.2342\n",
      "Epoch [389/1000], Loss: 678.9755\n",
      "Epoch [390/1000], Loss: 713.7452\n",
      "Epoch [391/1000], Loss: 686.3070\n",
      "Epoch [392/1000], Loss: 716.2684\n",
      "Epoch [393/1000], Loss: 977.6867\n",
      "Epoch [394/1000], Loss: 774.1792\n",
      "Epoch [395/1000], Loss: 930.8555\n",
      "Epoch [396/1000], Loss: 985.9324\n",
      "Epoch [397/1000], Loss: 668.1482\n",
      "Epoch [398/1000], Loss: 699.4255\n",
      "Epoch [399/1000], Loss: 671.9270\n",
      "Epoch [400/1000], Loss: 663.2197\n",
      "Epoch [401/1000], Loss: 969.8088\n",
      "Epoch [402/1000], Loss: 888.8571\n",
      "Epoch [403/1000], Loss: 655.9606\n",
      "Epoch [404/1000], Loss: 680.2837\n",
      "Epoch [405/1000], Loss: 662.2216\n",
      "Epoch [406/1000], Loss: 643.3386\n",
      "Epoch [407/1000], Loss: 783.5995\n",
      "Epoch [408/1000], Loss: 669.0618\n",
      "Epoch [409/1000], Loss: 695.7489\n",
      "Epoch [410/1000], Loss: 865.9131\n",
      "Epoch [411/1000], Loss: 697.0621\n",
      "Epoch [412/1000], Loss: 948.0406\n",
      "Epoch [413/1000], Loss: 992.6335\n",
      "Epoch [414/1000], Loss: 718.5192\n",
      "Epoch [415/1000], Loss: 753.0554\n",
      "Epoch [416/1000], Loss: 649.2746\n",
      "Epoch [417/1000], Loss: 939.1069\n",
      "Epoch [418/1000], Loss: 631.6066\n",
      "Epoch [419/1000], Loss: 708.7026\n",
      "Epoch [420/1000], Loss: 882.6033\n",
      "Epoch [421/1000], Loss: 698.2586\n",
      "Epoch [422/1000], Loss: 825.3421\n",
      "Epoch [423/1000], Loss: 711.1215\n",
      "Epoch [424/1000], Loss: 598.4234\n",
      "Epoch [425/1000], Loss: 625.0678\n",
      "Epoch [426/1000], Loss: 593.5376\n",
      "Epoch [427/1000], Loss: 646.1188\n",
      "Epoch [428/1000], Loss: 608.8967\n",
      "Epoch [429/1000], Loss: 613.1492\n",
      "Epoch [430/1000], Loss: 899.6819\n",
      "Epoch [431/1000], Loss: 619.9053\n",
      "Epoch [432/1000], Loss: 607.7820\n",
      "Epoch [433/1000], Loss: 835.8329\n",
      "Epoch [434/1000], Loss: 632.4294\n",
      "Epoch [435/1000], Loss: 558.5802\n",
      "Epoch [436/1000], Loss: 823.5763\n",
      "Epoch [437/1000], Loss: 553.2837\n",
      "Epoch [438/1000], Loss: 804.9305\n",
      "Epoch [439/1000], Loss: 860.9270\n",
      "Epoch [440/1000], Loss: 796.6809\n",
      "Epoch [441/1000], Loss: 556.3204\n",
      "Epoch [442/1000], Loss: 566.3011\n",
      "Epoch [443/1000], Loss: 634.9539\n",
      "Epoch [444/1000], Loss: 593.8325\n",
      "Epoch [445/1000], Loss: 590.9555\n",
      "Epoch [446/1000], Loss: 534.5976\n",
      "Epoch [447/1000], Loss: 799.5775\n",
      "Epoch [448/1000], Loss: 772.1011\n",
      "Epoch [449/1000], Loss: 648.7399\n",
      "Epoch [450/1000], Loss: 842.6249\n",
      "Epoch [451/1000], Loss: 864.3691\n",
      "Epoch [452/1000], Loss: 774.0588\n",
      "Epoch [453/1000], Loss: 782.9200\n",
      "Epoch [454/1000], Loss: 556.4795\n",
      "Epoch [455/1000], Loss: 537.8821\n",
      "Epoch [456/1000], Loss: 536.8693\n",
      "Epoch [457/1000], Loss: 555.4991\n",
      "Epoch [458/1000], Loss: 564.2582\n",
      "Epoch [459/1000], Loss: 798.9772\n",
      "Epoch [460/1000], Loss: 554.9760\n",
      "Epoch [461/1000], Loss: 596.0905\n",
      "Epoch [462/1000], Loss: 782.6679\n",
      "Epoch [463/1000], Loss: 605.5116\n",
      "Epoch [464/1000], Loss: 534.2308\n",
      "Epoch [465/1000], Loss: 619.4690\n",
      "Epoch [466/1000], Loss: 538.6239\n",
      "Epoch [467/1000], Loss: 546.8089\n",
      "Epoch [468/1000], Loss: 556.6663\n",
      "Epoch [469/1000], Loss: 791.7975\n",
      "Epoch [470/1000], Loss: 546.7385\n",
      "Epoch [471/1000], Loss: 750.7273\n",
      "Epoch [472/1000], Loss: 623.5412\n",
      "Epoch [473/1000], Loss: 756.8943\n",
      "Epoch [474/1000], Loss: 594.0234\n",
      "Epoch [475/1000], Loss: 519.4749\n",
      "Epoch [476/1000], Loss: 775.6706\n",
      "Epoch [477/1000], Loss: 722.2050\n",
      "Epoch [478/1000], Loss: 522.8086\n",
      "Epoch [479/1000], Loss: 742.0369\n",
      "Epoch [480/1000], Loss: 564.3589\n",
      "Epoch [481/1000], Loss: 549.2355\n",
      "Epoch [482/1000], Loss: 546.0281\n",
      "Epoch [483/1000], Loss: 481.4263\n",
      "Epoch [484/1000], Loss: 490.7368\n",
      "Epoch [485/1000], Loss: 511.3193\n",
      "Epoch [486/1000], Loss: 532.3472\n",
      "Epoch [487/1000], Loss: 525.0946\n",
      "Epoch [488/1000], Loss: 738.7598\n",
      "Epoch [489/1000], Loss: 701.2795\n",
      "Epoch [490/1000], Loss: 477.0151\n",
      "Epoch [491/1000], Loss: 722.8315\n",
      "Epoch [492/1000], Loss: 503.1584\n",
      "Epoch [493/1000], Loss: 470.2647\n",
      "Epoch [494/1000], Loss: 692.6913\n",
      "Epoch [495/1000], Loss: 554.5620\n",
      "Epoch [496/1000], Loss: 467.1438\n",
      "Epoch [497/1000], Loss: 540.5557\n",
      "Epoch [498/1000], Loss: 726.7705\n",
      "Epoch [499/1000], Loss: 673.7250\n",
      "Epoch [500/1000], Loss: 727.0251\n",
      "Epoch [501/1000], Loss: 497.0874\n",
      "Epoch [502/1000], Loss: 471.8330\n",
      "Epoch [503/1000], Loss: 492.0008\n",
      "Epoch [504/1000], Loss: 714.8741\n",
      "Epoch [505/1000], Loss: 486.7889\n",
      "Epoch [506/1000], Loss: 702.9281\n",
      "Epoch [507/1000], Loss: 516.4919\n",
      "Epoch [508/1000], Loss: 502.7413\n",
      "Epoch [509/1000], Loss: 524.0003\n",
      "Epoch [510/1000], Loss: 469.9784\n",
      "Epoch [511/1000], Loss: 517.5348\n",
      "Epoch [512/1000], Loss: 478.0624\n",
      "Epoch [513/1000], Loss: 505.1556\n",
      "Epoch [514/1000], Loss: 467.7964\n",
      "Epoch [515/1000], Loss: 665.6117\n",
      "Epoch [516/1000], Loss: 531.1664\n",
      "Epoch [517/1000], Loss: 714.8997\n",
      "Epoch [518/1000], Loss: 461.1474\n",
      "Epoch [519/1000], Loss: 657.3616\n",
      "Epoch [520/1000], Loss: 497.7197\n",
      "Epoch [521/1000], Loss: 703.2670\n",
      "Epoch [522/1000], Loss: 466.8362\n",
      "Epoch [523/1000], Loss: 479.8540\n",
      "Epoch [524/1000], Loss: 527.6545\n",
      "Epoch [525/1000], Loss: 458.2719\n",
      "Epoch [526/1000], Loss: 471.3285\n",
      "Epoch [527/1000], Loss: 643.5493\n",
      "Epoch [528/1000], Loss: 499.0700\n",
      "Epoch [529/1000], Loss: 688.4172\n",
      "Epoch [530/1000], Loss: 442.0292\n",
      "Epoch [531/1000], Loss: 500.7981\n",
      "Epoch [532/1000], Loss: 677.1091\n",
      "Epoch [533/1000], Loss: 505.6601\n",
      "Epoch [534/1000], Loss: 653.7203\n",
      "Epoch [535/1000], Loss: 645.2615\n",
      "Epoch [536/1000], Loss: 615.2953\n",
      "Epoch [537/1000], Loss: 437.3063\n",
      "Epoch [538/1000], Loss: 671.6714\n",
      "Epoch [539/1000], Loss: 431.1250\n",
      "Epoch [540/1000], Loss: 442.8476\n",
      "Epoch [541/1000], Loss: 645.8028\n",
      "Epoch [542/1000], Loss: 630.3595\n",
      "Epoch [543/1000], Loss: 688.0718\n",
      "Epoch [544/1000], Loss: 492.8854\n",
      "Epoch [545/1000], Loss: 702.8220\n",
      "Epoch [546/1000], Loss: 616.7093\n",
      "Epoch [547/1000], Loss: 616.8334\n",
      "Epoch [548/1000], Loss: 668.8847\n",
      "Epoch [549/1000], Loss: 455.5725\n",
      "Epoch [550/1000], Loss: 626.0604\n",
      "Epoch [551/1000], Loss: 424.1291\n",
      "Epoch [552/1000], Loss: 639.0404\n",
      "Epoch [553/1000], Loss: 444.2179\n",
      "Epoch [554/1000], Loss: 479.2318\n",
      "Epoch [555/1000], Loss: 648.9226\n",
      "Epoch [556/1000], Loss: 498.7453\n",
      "Epoch [557/1000], Loss: 610.2951\n",
      "Epoch [558/1000], Loss: 434.0574\n",
      "Epoch [559/1000], Loss: 399.0654\n",
      "Epoch [560/1000], Loss: 414.4438\n",
      "Epoch [561/1000], Loss: 457.1820\n",
      "Epoch [562/1000], Loss: 591.3372\n",
      "Epoch [563/1000], Loss: 441.5317\n",
      "Epoch [564/1000], Loss: 608.8033\n",
      "Epoch [565/1000], Loss: 407.7336\n",
      "Epoch [566/1000], Loss: 617.9383\n",
      "Epoch [567/1000], Loss: 444.8275\n",
      "Epoch [568/1000], Loss: 403.9206\n",
      "Epoch [569/1000], Loss: 437.2093\n",
      "Epoch [570/1000], Loss: 567.9735\n",
      "Epoch [571/1000], Loss: 391.4270\n",
      "Epoch [572/1000], Loss: 627.1944\n",
      "Epoch [573/1000], Loss: 440.6817\n",
      "Epoch [574/1000], Loss: 386.3377\n",
      "Epoch [575/1000], Loss: 571.5151\n",
      "Epoch [576/1000], Loss: 452.2171\n",
      "Epoch [577/1000], Loss: 380.0750\n",
      "Epoch [578/1000], Loss: 392.7143\n",
      "Epoch [579/1000], Loss: 587.8640\n",
      "Epoch [580/1000], Loss: 440.0987\n",
      "Epoch [581/1000], Loss: 420.6347\n",
      "Epoch [582/1000], Loss: 620.0965\n",
      "Epoch [583/1000], Loss: 433.5591\n",
      "Epoch [584/1000], Loss: 600.6492\n",
      "Epoch [585/1000], Loss: 372.6032\n",
      "Epoch [586/1000], Loss: 444.9844\n",
      "Epoch [587/1000], Loss: 393.9918\n",
      "Epoch [588/1000], Loss: 372.5100\n",
      "Epoch [589/1000], Loss: 394.1352\n",
      "Epoch [590/1000], Loss: 365.4034\n",
      "Epoch [591/1000], Loss: 561.1406\n",
      "Epoch [592/1000], Loss: 363.9823\n",
      "Epoch [593/1000], Loss: 551.2490\n",
      "Epoch [594/1000], Loss: 584.6075\n",
      "Epoch [595/1000], Loss: 552.2101\n",
      "Epoch [596/1000], Loss: 584.0659\n",
      "Epoch [597/1000], Loss: 586.2716\n",
      "Epoch [598/1000], Loss: 536.4262\n",
      "Epoch [599/1000], Loss: 359.4278\n",
      "Epoch [600/1000], Loss: 362.3553\n",
      "Epoch [601/1000], Loss: 560.3477\n",
      "Epoch [602/1000], Loss: 360.5441\n",
      "Epoch [603/1000], Loss: 564.9545\n",
      "Epoch [604/1000], Loss: 557.2078\n",
      "Epoch [605/1000], Loss: 397.1788\n",
      "Epoch [606/1000], Loss: 534.3028\n",
      "Epoch [607/1000], Loss: 366.8951\n",
      "Epoch [608/1000], Loss: 532.1038\n",
      "Epoch [609/1000], Loss: 527.1440\n",
      "Epoch [610/1000], Loss: 408.4947\n",
      "Epoch [611/1000], Loss: 393.6402\n",
      "Epoch [612/1000], Loss: 582.4009\n",
      "Epoch [613/1000], Loss: 511.1795\n",
      "Epoch [614/1000], Loss: 534.7783\n",
      "Epoch [615/1000], Loss: 347.1241\n",
      "Epoch [616/1000], Loss: 384.0825\n",
      "Epoch [617/1000], Loss: 418.0925\n",
      "Epoch [618/1000], Loss: 353.7696\n",
      "Epoch [619/1000], Loss: 395.3336\n",
      "Epoch [620/1000], Loss: 380.8561\n",
      "Epoch [621/1000], Loss: 334.7930\n",
      "Epoch [622/1000], Loss: 511.0338\n",
      "Epoch [623/1000], Loss: 486.0320\n",
      "Epoch [624/1000], Loss: 369.0717\n",
      "Epoch [625/1000], Loss: 339.2474\n",
      "Epoch [626/1000], Loss: 521.1011\n",
      "Epoch [627/1000], Loss: 354.7333\n",
      "Epoch [628/1000], Loss: 349.8464\n",
      "Epoch [629/1000], Loss: 378.0635\n",
      "Epoch [630/1000], Loss: 344.1298\n",
      "Epoch [631/1000], Loss: 387.5191\n",
      "Epoch [632/1000], Loss: 522.1043\n",
      "Epoch [633/1000], Loss: 534.1386\n",
      "Epoch [634/1000], Loss: 356.6430\n",
      "Epoch [635/1000], Loss: 336.9876\n",
      "Epoch [636/1000], Loss: 515.7875\n",
      "Epoch [637/1000], Loss: 378.7638\n",
      "Epoch [638/1000], Loss: 502.4372\n",
      "Epoch [639/1000], Loss: 502.1501\n",
      "Epoch [640/1000], Loss: 457.9767\n",
      "Epoch [641/1000], Loss: 355.4335\n",
      "Epoch [642/1000], Loss: 349.3158\n",
      "Epoch [643/1000], Loss: 349.1094\n",
      "Epoch [644/1000], Loss: 376.3652\n",
      "Epoch [645/1000], Loss: 484.5507\n",
      "Epoch [646/1000], Loss: 378.9776\n",
      "Epoch [647/1000], Loss: 353.4695\n",
      "Epoch [648/1000], Loss: 498.8406\n",
      "Epoch [649/1000], Loss: 453.8204\n",
      "Epoch [650/1000], Loss: 334.9749\n",
      "Epoch [651/1000], Loss: 358.7719\n",
      "Epoch [652/1000], Loss: 333.7188\n",
      "Epoch [653/1000], Loss: 485.1451\n",
      "Epoch [654/1000], Loss: 469.8497\n",
      "Epoch [655/1000], Loss: 346.3930\n",
      "Epoch [656/1000], Loss: 501.4284\n",
      "Epoch [657/1000], Loss: 345.6286\n",
      "Epoch [658/1000], Loss: 366.3686\n",
      "Epoch [659/1000], Loss: 351.8118\n",
      "Epoch [660/1000], Loss: 321.2089\n",
      "Epoch [661/1000], Loss: 374.5898\n",
      "Epoch [662/1000], Loss: 314.6265\n",
      "Epoch [663/1000], Loss: 353.8429\n",
      "Epoch [664/1000], Loss: 296.3785\n",
      "Epoch [665/1000], Loss: 475.0358\n",
      "Epoch [666/1000], Loss: 441.0430\n",
      "Epoch [667/1000], Loss: 298.7408\n",
      "Epoch [668/1000], Loss: 294.1883\n",
      "Epoch [669/1000], Loss: 329.7269\n",
      "Epoch [670/1000], Loss: 443.9625\n",
      "Epoch [671/1000], Loss: 364.8090\n",
      "Epoch [672/1000], Loss: 292.8962\n",
      "Epoch [673/1000], Loss: 468.6568\n",
      "Epoch [674/1000], Loss: 429.6815\n",
      "Epoch [675/1000], Loss: 462.5324\n",
      "Epoch [676/1000], Loss: 448.6741\n",
      "Epoch [677/1000], Loss: 450.7228\n",
      "Epoch [678/1000], Loss: 422.4635\n",
      "Epoch [679/1000], Loss: 304.2465\n",
      "Epoch [680/1000], Loss: 451.3939\n",
      "Epoch [681/1000], Loss: 335.8072\n",
      "Epoch [682/1000], Loss: 292.9057\n",
      "Epoch [683/1000], Loss: 329.7006\n",
      "Epoch [684/1000], Loss: 436.1728\n",
      "Epoch [685/1000], Loss: 291.5539\n",
      "Epoch [686/1000], Loss: 444.7980\n",
      "Epoch [687/1000], Loss: 298.6765\n",
      "Epoch [688/1000], Loss: 416.2425\n",
      "Epoch [689/1000], Loss: 360.9554\n",
      "Epoch [690/1000], Loss: 446.6164\n",
      "Epoch [691/1000], Loss: 416.8473\n",
      "Epoch [692/1000], Loss: 410.5189\n",
      "Epoch [693/1000], Loss: 290.6868\n",
      "Epoch [694/1000], Loss: 284.9585\n",
      "Epoch [695/1000], Loss: 319.7553\n",
      "Epoch [696/1000], Loss: 310.8732\n",
      "Epoch [697/1000], Loss: 284.4774\n",
      "Epoch [698/1000], Loss: 415.4095\n",
      "Epoch [699/1000], Loss: 424.7649\n",
      "Epoch [700/1000], Loss: 408.9492\n",
      "Epoch [701/1000], Loss: 318.6438\n",
      "Epoch [702/1000], Loss: 433.4249\n",
      "Epoch [703/1000], Loss: 329.1976\n",
      "Epoch [704/1000], Loss: 270.4727\n",
      "Epoch [705/1000], Loss: 298.9816\n",
      "Epoch [706/1000], Loss: 320.7250\n",
      "Epoch [707/1000], Loss: 417.1732\n",
      "Epoch [708/1000], Loss: 395.0503\n",
      "Epoch [709/1000], Loss: 376.4812\n",
      "Epoch [710/1000], Loss: 342.3588\n",
      "Epoch [711/1000], Loss: 294.8268\n",
      "Epoch [712/1000], Loss: 345.5048\n",
      "Epoch [713/1000], Loss: 267.6530\n",
      "Epoch [714/1000], Loss: 407.0942\n",
      "Epoch [715/1000], Loss: 377.9924\n",
      "Epoch [716/1000], Loss: 380.3470\n",
      "Epoch [717/1000], Loss: 389.1324\n",
      "Epoch [718/1000], Loss: 379.9020\n",
      "Epoch [719/1000], Loss: 358.1903\n",
      "Epoch [720/1000], Loss: 269.1484\n",
      "Epoch [721/1000], Loss: 398.4685\n",
      "Epoch [722/1000], Loss: 371.9136\n",
      "Epoch [723/1000], Loss: 280.2056\n",
      "Epoch [724/1000], Loss: 278.7992\n",
      "Epoch [725/1000], Loss: 417.7068\n",
      "Epoch [726/1000], Loss: 342.5328\n",
      "Epoch [727/1000], Loss: 334.3637\n",
      "Epoch [728/1000], Loss: 417.8103\n",
      "Epoch [729/1000], Loss: 343.0296\n",
      "Epoch [730/1000], Loss: 370.2916\n",
      "Epoch [731/1000], Loss: 240.0387\n",
      "Epoch [732/1000], Loss: 306.6727\n",
      "Epoch [733/1000], Loss: 247.3095\n",
      "Epoch [734/1000], Loss: 275.7610\n",
      "Epoch [735/1000], Loss: 375.6855\n",
      "Epoch [736/1000], Loss: 383.2784\n",
      "Epoch [737/1000], Loss: 247.7623\n",
      "Epoch [738/1000], Loss: 248.8334\n",
      "Epoch [739/1000], Loss: 235.3440\n",
      "Epoch [740/1000], Loss: 369.8482\n",
      "Epoch [741/1000], Loss: 232.9778\n",
      "Epoch [742/1000], Loss: 262.0007\n",
      "Epoch [743/1000], Loss: 242.6027\n",
      "Epoch [744/1000], Loss: 241.0189\n",
      "Epoch [745/1000], Loss: 274.3267\n",
      "Epoch [746/1000], Loss: 347.7644\n",
      "Epoch [747/1000], Loss: 367.1683\n",
      "Epoch [748/1000], Loss: 337.4168\n",
      "Epoch [749/1000], Loss: 227.8759\n",
      "Epoch [750/1000], Loss: 341.4044\n",
      "Epoch [751/1000], Loss: 321.1114\n",
      "Epoch [752/1000], Loss: 277.0268\n",
      "Epoch [753/1000], Loss: 236.1450\n",
      "Epoch [754/1000], Loss: 242.8151\n",
      "Epoch [755/1000], Loss: 327.4242\n",
      "Epoch [756/1000], Loss: 323.5468\n",
      "Epoch [757/1000], Loss: 218.7807\n",
      "Epoch [758/1000], Loss: 221.4833\n",
      "Epoch [759/1000], Loss: 337.7875\n",
      "Epoch [760/1000], Loss: 217.7227\n",
      "Epoch [761/1000], Loss: 346.9655\n",
      "Epoch [762/1000], Loss: 324.5037\n",
      "Epoch [763/1000], Loss: 358.7915\n",
      "Epoch [764/1000], Loss: 218.2433\n",
      "Epoch [765/1000], Loss: 249.0260\n",
      "Epoch [766/1000], Loss: 213.2658\n",
      "Epoch [767/1000], Loss: 257.8113\n",
      "Epoch [768/1000], Loss: 240.3381\n",
      "Epoch [769/1000], Loss: 325.1520\n",
      "Epoch [770/1000], Loss: 312.2661\n",
      "Epoch [771/1000], Loss: 256.0676\n",
      "Epoch [772/1000], Loss: 233.7559\n",
      "Epoch [773/1000], Loss: 253.1933\n",
      "Epoch [774/1000], Loss: 313.4827\n",
      "Epoch [775/1000], Loss: 327.5912\n",
      "Epoch [776/1000], Loss: 302.0586\n",
      "Epoch [777/1000], Loss: 327.7046\n",
      "Epoch [778/1000], Loss: 310.7026\n",
      "Epoch [779/1000], Loss: 296.3343\n",
      "Epoch [780/1000], Loss: 299.9542\n",
      "Epoch [781/1000], Loss: 301.6306\n",
      "Epoch [782/1000], Loss: 211.1175\n",
      "Epoch [783/1000], Loss: 295.4656\n",
      "Epoch [784/1000], Loss: 255.7541\n",
      "Epoch [785/1000], Loss: 234.2942\n",
      "Epoch [786/1000], Loss: 251.6182\n",
      "Epoch [787/1000], Loss: 197.8072\n",
      "Epoch [788/1000], Loss: 288.9898\n",
      "Epoch [789/1000], Loss: 285.4937\n",
      "Epoch [790/1000], Loss: 212.8885\n",
      "Epoch [791/1000], Loss: 296.9118\n",
      "Epoch [792/1000], Loss: 272.4637\n",
      "Epoch [793/1000], Loss: 266.2157\n",
      "Epoch [794/1000], Loss: 200.6810\n",
      "Epoch [795/1000], Loss: 209.3822\n",
      "Epoch [796/1000], Loss: 322.3007\n",
      "Epoch [797/1000], Loss: 198.6614\n",
      "Epoch [798/1000], Loss: 228.5812\n",
      "Epoch [799/1000], Loss: 231.4210\n",
      "Epoch [800/1000], Loss: 185.2192\n",
      "Epoch [801/1000], Loss: 285.1106\n",
      "Epoch [802/1000], Loss: 214.2369\n",
      "Epoch [803/1000], Loss: 288.9753\n",
      "Epoch [804/1000], Loss: 280.4933\n",
      "Epoch [805/1000], Loss: 217.6885\n",
      "Epoch [806/1000], Loss: 211.5842\n",
      "Epoch [807/1000], Loss: 308.3872\n",
      "Epoch [808/1000], Loss: 273.2981\n",
      "Epoch [809/1000], Loss: 309.8332\n",
      "Epoch [810/1000], Loss: 236.8467\n",
      "Epoch [811/1000], Loss: 186.7172\n",
      "Epoch [812/1000], Loss: 264.1031\n",
      "Epoch [813/1000], Loss: 188.3784\n",
      "Epoch [814/1000], Loss: 177.6226\n",
      "Epoch [815/1000], Loss: 176.7151\n",
      "Epoch [816/1000], Loss: 204.8869\n",
      "Epoch [817/1000], Loss: 180.7255\n",
      "Epoch [818/1000], Loss: 177.1554\n",
      "Epoch [819/1000], Loss: 175.2372\n",
      "Epoch [820/1000], Loss: 200.7756\n",
      "Epoch [821/1000], Loss: 200.4013\n",
      "Epoch [822/1000], Loss: 170.9962\n",
      "Epoch [823/1000], Loss: 276.3807\n",
      "Epoch [824/1000], Loss: 169.7082\n",
      "Epoch [825/1000], Loss: 267.5342\n",
      "Epoch [826/1000], Loss: 218.4853\n",
      "Epoch [827/1000], Loss: 190.9305\n",
      "Epoch [828/1000], Loss: 183.5032\n",
      "Epoch [829/1000], Loss: 237.1883\n",
      "Epoch [830/1000], Loss: 207.5142\n",
      "Epoch [831/1000], Loss: 174.5250\n",
      "Epoch [832/1000], Loss: 205.9543\n",
      "Epoch [833/1000], Loss: 244.0954\n",
      "Epoch [834/1000], Loss: 163.9229\n",
      "Epoch [835/1000], Loss: 163.4899\n",
      "Epoch [836/1000], Loss: 257.7026\n",
      "Epoch [837/1000], Loss: 169.6941\n",
      "Epoch [838/1000], Loss: 168.0151\n",
      "Epoch [839/1000], Loss: 252.0596\n",
      "Epoch [840/1000], Loss: 181.1051\n",
      "Epoch [841/1000], Loss: 169.6229\n",
      "Epoch [842/1000], Loss: 279.4336\n",
      "Epoch [843/1000], Loss: 265.5739\n",
      "Epoch [844/1000], Loss: 194.2728\n",
      "Epoch [845/1000], Loss: 181.0050\n",
      "Epoch [846/1000], Loss: 215.0288\n",
      "Epoch [847/1000], Loss: 240.4631\n",
      "Epoch [848/1000], Loss: 237.3136\n",
      "Epoch [849/1000], Loss: 165.5064\n",
      "Epoch [850/1000], Loss: 256.9649\n",
      "Epoch [851/1000], Loss: 161.2108\n",
      "Epoch [852/1000], Loss: 160.8705\n",
      "Epoch [853/1000], Loss: 193.7097\n",
      "Epoch [854/1000], Loss: 176.3775\n",
      "Epoch [855/1000], Loss: 158.5798\n",
      "Epoch [856/1000], Loss: 202.9944\n",
      "Epoch [857/1000], Loss: 159.5039\n",
      "Epoch [858/1000], Loss: 168.7441\n",
      "Epoch [859/1000], Loss: 254.3782\n",
      "Epoch [860/1000], Loss: 153.6403\n",
      "Epoch [861/1000], Loss: 236.8409\n",
      "Epoch [862/1000], Loss: 241.3090\n",
      "Epoch [863/1000], Loss: 145.2475\n",
      "Epoch [864/1000], Loss: 167.1916\n",
      "Epoch [865/1000], Loss: 200.5674\n",
      "Epoch [866/1000], Loss: 157.6729\n",
      "Epoch [867/1000], Loss: 148.6740\n",
      "Epoch [868/1000], Loss: 222.5151\n",
      "Epoch [869/1000], Loss: 207.5263\n",
      "Epoch [870/1000], Loss: 204.2556\n",
      "Epoch [871/1000], Loss: 222.9728\n",
      "Epoch [872/1000], Loss: 223.0466\n",
      "Epoch [873/1000], Loss: 216.2372\n",
      "Epoch [874/1000], Loss: 193.2176\n",
      "Epoch [875/1000], Loss: 140.9100\n",
      "Epoch [876/1000], Loss: 140.9496\n",
      "Epoch [877/1000], Loss: 137.9291\n",
      "Epoch [878/1000], Loss: 137.2149\n",
      "Epoch [879/1000], Loss: 213.2384\n",
      "Epoch [880/1000], Loss: 165.3425\n",
      "Epoch [881/1000], Loss: 134.8970\n",
      "Epoch [882/1000], Loss: 134.3735\n",
      "Epoch [883/1000], Loss: 133.4667\n",
      "Epoch [884/1000], Loss: 132.4672\n",
      "Epoch [885/1000], Loss: 141.3874\n",
      "Epoch [886/1000], Loss: 178.8675\n",
      "Epoch [887/1000], Loss: 166.0360\n",
      "Epoch [888/1000], Loss: 138.1002\n",
      "Epoch [889/1000], Loss: 200.7228\n",
      "Epoch [890/1000], Loss: 189.3727\n",
      "Epoch [891/1000], Loss: 150.3694\n",
      "Epoch [892/1000], Loss: 177.1368\n",
      "Epoch [893/1000], Loss: 165.4999\n",
      "Epoch [894/1000], Loss: 152.7472\n",
      "Epoch [895/1000], Loss: 202.0535\n",
      "Epoch [896/1000], Loss: 139.7892\n",
      "Epoch [897/1000], Loss: 158.6118\n",
      "Epoch [898/1000], Loss: 178.6045\n",
      "Epoch [899/1000], Loss: 177.8766\n",
      "Epoch [900/1000], Loss: 142.0014\n",
      "Epoch [901/1000], Loss: 158.4175\n",
      "Epoch [902/1000], Loss: 159.7391\n",
      "Epoch [903/1000], Loss: 126.2005\n",
      "Epoch [904/1000], Loss: 190.8774\n",
      "Epoch [905/1000], Loss: 139.4364\n",
      "Epoch [906/1000], Loss: 128.0950\n",
      "Epoch [907/1000], Loss: 132.1566\n",
      "Epoch [908/1000], Loss: 149.2142\n",
      "Epoch [909/1000], Loss: 202.8542\n",
      "Epoch [910/1000], Loss: 130.6713\n",
      "Epoch [911/1000], Loss: 180.8266\n",
      "Epoch [912/1000], Loss: 186.9838\n",
      "Epoch [913/1000], Loss: 120.6237\n",
      "Epoch [914/1000], Loss: 175.4569\n",
      "Epoch [915/1000], Loss: 114.5692\n",
      "Epoch [916/1000], Loss: 163.1713\n",
      "Epoch [917/1000], Loss: 183.6191\n",
      "Epoch [918/1000], Loss: 175.1518\n",
      "Epoch [919/1000], Loss: 175.4435\n",
      "Epoch [920/1000], Loss: 112.7999\n",
      "Epoch [921/1000], Loss: 177.3530\n",
      "Epoch [922/1000], Loss: 156.5143\n",
      "Epoch [923/1000], Loss: 143.2467\n",
      "Epoch [924/1000], Loss: 133.0903\n",
      "Epoch [925/1000], Loss: 170.4515\n",
      "Epoch [926/1000], Loss: 124.2943\n",
      "Epoch [927/1000], Loss: 130.5623\n",
      "Epoch [928/1000], Loss: 175.7093\n",
      "Epoch [929/1000], Loss: 126.7232\n",
      "Epoch [930/1000], Loss: 125.6238\n",
      "Epoch [931/1000], Loss: 117.5643\n",
      "Epoch [932/1000], Loss: 177.8084\n",
      "Epoch [933/1000], Loss: 124.7491\n",
      "Epoch [934/1000], Loss: 116.2528\n",
      "Epoch [935/1000], Loss: 140.0003\n",
      "Epoch [936/1000], Loss: 155.6084\n",
      "Epoch [937/1000], Loss: 136.8001\n",
      "Epoch [938/1000], Loss: 121.0601\n",
      "Epoch [939/1000], Loss: 113.0960\n",
      "Epoch [940/1000], Loss: 140.4515\n",
      "Epoch [941/1000], Loss: 124.9130\n",
      "Epoch [942/1000], Loss: 164.7642\n",
      "Epoch [943/1000], Loss: 104.9237\n",
      "Epoch [944/1000], Loss: 151.4295\n",
      "Epoch [945/1000], Loss: 129.5562\n",
      "Epoch [946/1000], Loss: 99.3202\n",
      "Epoch [947/1000], Loss: 134.5352\n",
      "Epoch [948/1000], Loss: 114.5222\n",
      "Epoch [949/1000], Loss: 144.6022\n",
      "Epoch [950/1000], Loss: 143.3341\n",
      "Epoch [951/1000], Loss: 155.4815\n",
      "Epoch [952/1000], Loss: 131.7679\n",
      "Epoch [953/1000], Loss: 115.2878\n",
      "Epoch [954/1000], Loss: 109.2762\n",
      "Epoch [955/1000], Loss: 126.6299\n",
      "Epoch [956/1000], Loss: 137.9083\n",
      "Epoch [957/1000], Loss: 114.9430\n",
      "Epoch [958/1000], Loss: 96.6367\n",
      "Epoch [959/1000], Loss: 132.4774\n",
      "Epoch [960/1000], Loss: 113.5347\n",
      "Epoch [961/1000], Loss: 144.6714\n",
      "Epoch [962/1000], Loss: 133.7979\n",
      "Epoch [963/1000], Loss: 99.3907\n",
      "Epoch [964/1000], Loss: 122.2119\n",
      "Epoch [965/1000], Loss: 92.9340\n",
      "Epoch [966/1000], Loss: 139.7727\n",
      "Epoch [967/1000], Loss: 119.6281\n",
      "Epoch [968/1000], Loss: 100.0176\n",
      "Epoch [969/1000], Loss: 112.8904\n",
      "Epoch [970/1000], Loss: 131.1703\n",
      "Epoch [971/1000], Loss: 91.4738\n",
      "Epoch [972/1000], Loss: 91.2020\n",
      "Epoch [973/1000], Loss: 92.0626\n",
      "Epoch [974/1000], Loss: 91.9599\n",
      "Epoch [975/1000], Loss: 86.6431\n",
      "Epoch [976/1000], Loss: 88.2544\n",
      "Epoch [977/1000], Loss: 84.2026\n",
      "Epoch [978/1000], Loss: 86.3832\n",
      "Epoch [979/1000], Loss: 119.6877\n",
      "Epoch [980/1000], Loss: 88.5928\n",
      "Epoch [981/1000], Loss: 114.9536\n",
      "Epoch [982/1000], Loss: 87.5819\n",
      "Epoch [983/1000], Loss: 104.3137\n",
      "Epoch [984/1000], Loss: 126.2342\n",
      "Epoch [985/1000], Loss: 101.7772\n",
      "Epoch [986/1000], Loss: 140.0965\n",
      "Epoch [987/1000], Loss: 146.9243\n",
      "Epoch [988/1000], Loss: 116.1897\n",
      "Epoch [989/1000], Loss: 124.0378\n",
      "Epoch [990/1000], Loss: 137.8405\n",
      "Epoch [991/1000], Loss: 134.0923\n",
      "Epoch [992/1000], Loss: 102.0394\n",
      "Epoch [993/1000], Loss: 113.9087\n",
      "Epoch [994/1000], Loss: 99.9788\n",
      "Epoch [995/1000], Loss: 95.3479\n",
      "Epoch [996/1000], Loss: 88.7510\n",
      "Epoch [997/1000], Loss: 89.4315\n",
      "Epoch [998/1000], Loss: 103.4755\n",
      "Epoch [999/1000], Loss: 99.9205\n",
      "Epoch [1000/1000], Loss: 81.1723\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: [ 874. 1251.], Predicted: [ 873.7272 1187.0973]\n",
      "True: [759. 894.], Predicted: [782.4226  943.75964]\n",
      "True: [ 885. 1075.], Predicted: [ 887.7649 1076.9205]\n",
      "True: [ 897. 1095.], Predicted: [ 898.4181 1092.9603]\n",
      "True: [ 909. 1112.], Predicted: [ 909.65405 1109.308  ]\n",
      "True: [ 922. 1131.], Predicted: [ 923.1651 1131.4318]\n",
      "True: [ 932. 1150.], Predicted: [ 930.1482 1141.2787]\n",
      "True: [ 947. 1165.], Predicted: [ 946.23413 1166.8523 ]\n",
      "True: [ 957. 1184.], Predicted: [ 960.1891 1186.9962]\n",
      "True: [ 967. 1198.], Predicted: [ 965.09576 1194.1664 ]\n",
      "True: [ 981. 1217.], Predicted: [ 977.06464 1210.7899 ]\n",
      "True: [ 988. 1232.], Predicted: [ 989.1035 1227.6542]\n",
      "True: [774. 915.], Predicted: [778.44006 933.1844 ]\n",
      "True: [ 999. 1247.], Predicted: [1000.2354 1245.9167]\n",
      "True: [1007. 1262.], Predicted: [1010.62305 1266.0848 ]\n",
      "True: [1015. 1277.], Predicted: [1017.8845 1272.5126]\n",
      "True: [1028. 1292.], Predicted: [1025.0977 1286.0288]\n",
      "True: [1038. 1305.], Predicted: [1033.9382 1300.0475]\n",
      "True: [1044. 1318.], Predicted: [1042.159  1313.7511]\n",
      "True: [1054. 1331.], Predicted: [1048.4342 1326.0192]\n",
      "True: [1061. 1345.], Predicted: [1060.5326 1345.3427]\n",
      "True: [1070. 1358.], Predicted: [1066.1234 1355.246 ]\n",
      "True: [1075. 1371.], Predicted: [1072.5532 1363.705 ]\n",
      "True: [791. 935.], Predicted: [782.68286 933.9263 ]\n",
      "True: [1082. 1381.], Predicted: [1081.6086 1379.2444]\n",
      "True: [1088. 1394.], Predicted: [1089.9641 1392.8534]\n",
      "True: [1097. 1402.], Predicted: [1093.6274 1400.0327]\n",
      "True: [1101. 1411.], Predicted: [1099.8022 1408.6512]\n",
      "True: [1108. 1424.], Predicted: [1106.017  1418.1649]\n",
      "True: [1112. 1434.], Predicted: [1111.1829 1431.3901]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataset, num_samples=30):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            img, label = dataset[i]\n",
    "            img = img.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Model prediction\n",
    "            prediction = model(img)\n",
    "            print(f\"True: {label.numpy()}, Predicted: {prediction.numpy().flatten()}\")\n",
    "\n",
    "# Test on some samples\n",
    "evaluate(model, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './fan_corner_detector.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15080\\2217343306.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('fan_corner_detector.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNNPointDetector(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNPointDetector()  # 重新初始化模型\n",
    "model.load_state_dict(torch.load('fan_corner_detector.pth'))\n",
    "model.eval()  # 切换到评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvdi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
